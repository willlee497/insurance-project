{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "AnB7bouiHz5j",
        "outputId": "4d8b96bb-61a4-4c13-80a6-21b5faff619e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned CSV saved as 'final_cleaned.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5f1e8885-af4e-4c54-9179-8690b9730e22\", \"final_cleaned.csv\", 3476914)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Load the raw data\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# 1. Drop exact duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 2. Drop duplicate complaints based on 'File No.'\n",
        "df = df.drop_duplicates(subset=['File No.'], keep='first')\n",
        "\n",
        "# 3. Group rare categories in categorical columns under 'Other'\n",
        "def group_rare_categories(df, threshold=0.02):\n",
        "    \"\"\"\n",
        "    Replaces rare categories (below threshold) in object columns with 'Other'.\n",
        "    \"\"\"\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        freq = df[col].value_counts(normalize=True)\n",
        "        rare = freq[freq < threshold].index\n",
        "        df[col] = df[col].replace(rare, 'Other')\n",
        "    return df\n",
        "\n",
        "df = group_rare_categories(df)\n",
        "\n",
        "# 4. Convert 'Recovery' to numeric if it's not already\n",
        "df['Recovery'] = pd.to_numeric(df['Recovery'], errors='coerce')\n",
        "\n",
        "# 5. Drop rows that are completely empty (if any)\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "# 6. Save the cleaned DataFrame\n",
        "df.to_csv(\"final_cleaned.csv\", index=False)\n",
        "print(\"✅ Cleaned CSV saved as 'final_cleaned.csv'\")\n",
        "files.download(\"final_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why the Cleaning Code Works:\n",
        "\n",
        "**1. Rare Category Grouping**\n",
        "\n",
        "Our insurance data has tons of random company names, subcoverages, and disposition outcomes.\n",
        "Grouping anything that appears in <2% of the data into 'Other' keeps our model from getting overwhelmed by noise.\n",
        "\n",
        "\n",
        "**2. Duplicates (based on 'File No.') Removal:**\n",
        "\n",
        "Complaints might be logged multiple times or appear with slightly different info.\n",
        "Dropping extras avoids inflating trends.\n",
        "\n",
        "\n",
        "**3. Keeping Outliers (Recovery):**\n",
        "\n",
        "In insurance, outliers are the story.\n",
        "Settlements are either:\n",
        "\n",
        "0 dollars or up to 843,000 dollars in settlements.\n",
        "\n",
        "**For regression:** the outliers are our targets\n",
        "\n",
        "**Doing anomaly detection:** the outliers are the anomalies\n",
        "\n",
        "**clustering:** the outliers will shift clusters, which is fine for our usage.\n",
        "-> In clustering (like k-means), outliers can distort how clusters are formed.\n",
        "But in our case — insurance data — that’s not a bad thing.\n",
        "\n",
        "Example:\n",
        "\n",
        "A bunch of 0 or $100 settlements might group together.\n",
        "\n",
        "But a $500,000+ recovery could form its own cluster — that’s real signal, not noise.\n",
        "\n",
        "So instead of filtering them out, we let them stay, because they represent:\n",
        "\n",
        "Large recoveries\n",
        "\n",
        "Possibly complex or escalated complaints\n",
        "\n",
        "Things our ML models should be aware of\n",
        "\n",
        "We retained outliers in recovery amounts during clustering because extreme values are inherent to insurance claims. These high-recovery cases often reflect real-world escalation or rare scenarios and may naturally form distinct clusters. Removing them would obscure important patterns.\n",
        "We should not remove outliers for this use case unless we're testing a “norm-only” model for comparison.\n",
        "\n",
        "4. Keeping Missing Data Imputation Later:\n",
        "We didn’t impute here, which is actually great for us because our notebook will handle it with logic later:\n",
        "\n",
        "E.X:\n",
        "\n",
        "50% missing → drop the column\n",
        "\n",
        "mean/Unknown fill elsewhere\n",
        "\n",
        "\n",
        "this cleaned CSV isn’t “perfect” in the textbook sense — but it’s tailored for insurance ML workflows, which is more important.\n",
        "We preserved important variation and didn’t over-clean, which a lot of people mess up.\n"
      ],
      "metadata": {
        "id": "wWPuH1FbIyRb"
      }
    }
  ]
}